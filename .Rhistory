# Calculate the correlation between the predicted and real values with selected features
correlation_selected <- cor(predicted_values_selected, test_data$foreclosure_pc_2020)
# Calculate the mean squared error between the predicted and real values with selected features
mse_selected <- mean((predicted_values_selected - test_data$foreclosure_pc_2020)^2)
# Obtain coefficients for each selected feature
coefficients_selected <- coef(lm_model_selected)
print(coefficients_selected)
# Print the selected features, correlation, and mean squared error
cat("Selected features:", selected_features, "\n")
cat("Correlation between predicted and real values with selected features:", correlation_selected, "\n")
cat("Mean Squared Error with selected features:", mse_selected, "\n")
# Define test variables
test_variables_3 <- c("mortgaged_2010", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"foreclosure_pc_2010", "pct_built_2000_2009", "poverty_2010", "nhwhite_2020",
"mortgage_change_2010_2020", "poverty_change_2010_2020", "nhwhite_change_2010_2020",
"medincome_change_2010_2015", "medincome_change_2015_2020", "medincome_change_2010_2020",
"pop_change_pct")
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- createDataPartition(pg_foreclosures_per_tract$foreclosure_pc_2020, p = 0.8, list = FALSE)
train_data_6 <- pg_foreclosures_per_tract[train_indices, ]
test_data_6 <- pg_foreclosures_per_tract[-train_indices, ]
# Define control parameters for RFE
ctrl <- rfeControl(functions = lmFuncs, method = "cv", number = 10)
# Perform RFE on new training data excluding tract_number
lm_model_rfe <- rfe(x = train_data_6[, test_variables_3],
y = train_data_6$foreclosure_pc_2020,
sizes = c(1:15),
rfeControl = ctrl)
# Extract the selected features
selected_features <- predictors(lm_model_rfe)
# Train the linear regression model on the new training data with selected features
lm_model_selected <- lm(foreclosure_pc_2020 ~ . - 1, data = train_data_6[, c("foreclosure_pc_2020", selected_features)])
# Use the trained model to predict on the testing data with selected features
predicted_values_selected <- predict(lm_model_selected, newdata = test_data_6[, c(selected_features)])
# Calculate the correlation between the predicted and real values with selected features
correlation_selected <- cor(predicted_values_selected, test_data_6$foreclosure_pc_2020)
# Calculate the mean squared error between the predicted and real values with selected features
mse_selected <- mean((predicted_values_selected - test_data_6$foreclosure_pc_2020)^2)
# Obtain coefficients for each selected feature
coefficients_selected <- coef(lm_model_selected)
print(coefficients_selected)
# Print the selected features, correlation, and mean squared error
cat("Selected features:", selected_features, "\n")
cat("Correlation between predicted and real values with selected features:", correlation_selected, "\n")
cat("Mean Squared Error with selected features:", mse_selected, "\n")
# Define independent variables
independent_variables <- c(
"avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later",
"pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999",
"pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960",
"pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd",
"poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020",
"ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020",
"ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015",
"medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- createDataPartition(pg_foreclosures_per_tract$foreclosure_pc_2020, p = 0.8, list = FALSE)
train_data_7 <- pg_foreclosures_per_tract[train_indices, ]
test_data_7 <- pg_foreclosures_per_tract[-train_indices, ]
# Define control parameters for RFE
ctrl <- rfeControl(functions = lmFuncs, method = "cv", number = 10)
# Perform RFE on new training data using independent_variables
lm_model_rfe <- rfe(x = train_data_7[, independent_variables],
y = train_data_7$foreclosure_pc_2020,
sizes = c(1:15),
rfeControl = ctrl)
# Extract the selected features
selected_features <- predictors(lm_model_rfe)
# Train the linear regression model on the new training data with selected features
lm_model_selected <- lm(foreclosure_pc_2020 ~ . - 1, data = train_data_7[, c("foreclosure_pc_2020", selected_features)])
# Use the trained model to predict on the testing data with selected features
predicted_values_selected <- predict(lm_model_selected, newdata = test_data_7[, c(selected_features)])
# Calculate the correlation between the predicted and real values with selected features
correlation_selected <- cor(predicted_values_selected, test_data_7$foreclosure_pc_2020)
# Calculate the mean squared error between the predicted and real values with selected features
mse_selected <- mean((predicted_values_selected - test_data_7$foreclosure_pc_2020)^2)
# Obtain coefficients for each selected feature
coefficients_selected <- coef(lm_model_selected)
print(coefficients_selected)
# Print the selected features, correlation, and mean squared error
cat("Selected features:", selected_features, "\n")
cat("Correlation between predicted and real values with selected features:", correlation_selected, "\n")
cat("Mean Squared Error with selected features:", mse_selected, "\n")
# List of selected features obtained from RFE
selected_features <- c(
"pct_built_2020_later", "medincome_change_2015_2020", "mortgaged_2020",
"medincome_change_2010_2015", "pct_0_bd", "mortgaged_2010",
"pct_built_2010_2019", "ownoccupied_2010", "poverty_2010", "nhwhite_2010",
"pct_built_2000_2009", "pct_built_1990_1999", "poverty_2020",
"pct_built_1970_1979", "medincome_change_2010_2020", "pop_change_pct",
"pct_1_bd", "ownoccupied_2020", "mortgaged_2015", "pct_built_1980_1989",
"ownoccupied_2015", "nhwhite_2020", "pct_2_bd", "avg_bed", "pct_3_bd",
"foreclosure_pc_2010", "tract_medage_2020", "tract_medincome_2010",
"tract_medincome_2020", "tract_homevalue_2020", "mortgage_change_2010_2015",
"mortgage_change_2010_2020", "mortgage_change_2015_2020", "nhwhite_change_2010_2020",
"ownoccupied_change_2010_2015", "ownoccupied_change_2010_2020",
"ownoccupied_change_2015_2020", "pct_4_more_bd", "pct_built_pre_1960",
"poverty_change_2010_2020"
)
# List of independent variables
independent_variables <- c(
"avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later",
"pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999",
"pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960",
"pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd",
"poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020",
"ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020",
"ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015",
"medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)
# Identify variables in selected_features that are not in independent_variables
additional_features <- setdiff(selected_features, independent_variables)
print(additional_features)
# Define independent variables
independent_variables <- c(
"avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later",
"pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999",
"pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960",
"pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd",
"poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020",
"ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020",
"ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015",
"medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- createDataPartition(pg_foreclosures_per_tract$foreclosure_pc_2020, p = 0.8, list = FALSE)
train_data_7 <- pg_foreclosures_per_tract[train_indices, ]
test_data_7 <- pg_foreclosures_per_tract[-train_indices, ]
# Calculate VIF
vif_values <- vif(lm(foreclosure_pc_2020 ~ ., data = train_data_7[, independent_variables]))
View(train_data_7)
# Define independent variables
independent_variables <- c(
"avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later",
"pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999",
"pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960",
"pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd",
"poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020",
"ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020",
"ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015",
"medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- createDataPartition(pg_foreclosures_per_tract$foreclosure_pc_2020, p = 0.8, list = FALSE)
train_data_7 <- pg_foreclosures_per_tract[train_indices, ]
test_data_7 <- pg_foreclosures_per_tract[-train_indices, ]
# Calculate VIF
vif_values <- vif(lm(foreclosure_pc_2020 ~ ., data = train_data_7[, c("foreclosure_pc_2020", independent_variables)]))
# Define independent variables
independent_variables <- c(
"avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later",
"pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999",
"pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960",
"pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd",
"poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020",
"ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020",
"ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015",
"medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- createDataPartition(pg_foreclosures_per_tract$foreclosure_pc_2020, p = 0.8, list = FALSE)
train_data_7 <- pg_foreclosures_per_tract[train_indices, ]
test_data_7 <- pg_foreclosures_per_tract[-train_indices, ]
# Perform PCA to identify linear dependencies
pca_result <- prcomp(train_data_7[, independent_variables], center = TRUE, scale. = TRUE)
# Extract rotation matrix from PCA result
rotation_matrix <- pca_result$rotation
# Identify linearly dependent predictors based on rotation matrix
dependent_predictors <- colnames(train_data_7[, independent_variables])[rowSums(abs(rotation_matrix) > 1e-10) > 1]
# Remove linearly dependent predictors
independent_variables_filtered <- setdiff(independent_variables, dependent_predictors)
# Calculate VIF with filtered independent variables
vif_values <- vif(lm(foreclosure_pc_2020 ~ ., data = train_data_7[, c("foreclosure_pc_2020", independent_variables_filtered)]))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
library(janitor)
stand_dests_df <- read_csv("archive/standardize_sponsors_dests/standardizing_dests/house_12to23_w_stand_dests.csv")
stand_dests_df
sponsor_list <- read_delim("archive/standardize_sponsors_dests/standardized_sponsors/open_refined_sponsors.csv")
house_travel <- stand_dests_df %>%
full_join(sponsor_list)
# Convert "ReturnDate" to Date type
house_travel$ReturnDate <- as.Date(house_travel$ReturnDate, format = "%m/%d/%Y")
# Extract the year from "DepartureDate" and populate "Year" column
house_travel$Year <- year(house_travel$DepartureDate)
# Extract the year/month combination from "DepartureDate" and populate "Year" column
house_travel$year_month <- paste(year(house_travel$DepartureDate), month(house_travel$DepartureDate, label = TRUE), sep = "-")
# Filter out non-members or staffers
house_travel <- house_travel %>%
filter(!is.na(State) & !is.na(District))
# Print or use house_travel as needed
print(house_travel)
# Identify all pairs in the house_travel dataset that have matching DocIDs and save those with matching DocIDs for further review.
duplicate_docids_house_travel <- house_travel %>%
group_by(DocID) %>%
filter(n() > 1)
combined_duplicate_docids <- duplicate_docids_house_travel %>%
group_by(DocID) %>%
summarise(
across(
-c(DepartureDate, ReturnDate),
~ ifelse(
length(unique(na.omit(.))) > 1,
paste(na.omit(.), collapse = "; "),
first(na.omit(.))
)
),
DepartureDate = min(DepartureDate, na.rm = TRUE), #This is an imperfect method of combining travel date information, but because these forms have identical date information, it works.
ReturnDate = max(ReturnDate, na.rm = TRUE)
)
filtered_house_travel_draft <- house_travel %>%
anti_join(duplicate_docids_house_travel, by = "DocID")
# Identify duplicate rows based on the combination of values in specified columns that remain consistent from form to form.
# To cast a wide net, we create one version that identifies rows with matching values in the FilerName, MemberName, State, Year and standardized_dest columns -- columns that consistently match from entry to entry.
columns_wide <- c("FilerName", "MemberName", "State", "Year", "standardized_dest")
# To cast a narrower net, we create another version that identifies rows with matching values in the FilerName, MemberName, State, and standardized_dest columns, as well as the year_month and sponsors_cleaned columns -- columns that may change within an otherwise matching set.
columns_narrow <- c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned")
duplicates_logical_wide <- duplicated(filtered_house_travel_draft[columns_wide], fromLast = TRUE) | duplicated(filtered_house_travel_draft[columns_wide])
duplicates_logical_narrow <- duplicated(filtered_house_travel_draft[columns_narrow], fromLast = TRUE) | duplicated(filtered_house_travel_draft[columns_narrow])
# Subset the original dataset using the wide and narrow duplicate vectors to get the duplicate rows
duplicates_wide <- subset(filtered_house_travel_draft, duplicates_logical_wide)
duplicates_narrow <- subset(filtered_house_travel_draft, duplicates_logical_narrow)
# Arrange the rows to consolidate matching groups.
duplicates_wide <- arrange(duplicates_wide, FilerName, MemberName, State, Year, standardized_dest)
duplicates_narrow <- arrange(duplicates_narrow, FilerName, MemberName, State, Year, standardized_dest, sponsors_cleaned)
missing <- anti_join(duplicates_narrow, duplicates_wide, by = c("DocID"))
# Use the DocID column to identify the most recent version of a travel filing.
rows_to_keep <- duplicates_narrow %>%
group_by(FilerName, MemberName, State, year_month, standardized_dest, sponsors_cleaned) %>%
filter(DocID == max(DocID))
diff_sponsor_month <- anti_join(duplicates_wide, duplicates_narrow, by = c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned"))
no_change_diff_sponsor_month_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() == 1) %>%
ungroup()
consolidated_diff_sponsor_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() > 1) %>%
slice_max(order_by = DocID) %>%
ungroup()
filtered_house_travel_draft <- anti_join(filtered_house_travel_draft, duplicates_wide, by = c("DocID"))
# Convert 'index_number' to numeric in combined_duplicate_docids
combined_duplicate_docids <- mutate(combined_duplicate_docids, index_number = as.numeric(index_number))
# Now you can bind the rows
filtered_house_travel_draft <- bind_rows(filtered_house_travel_draft, rows_to_keep)
filtered_house_travel_draft <- bind_rows(filtered_house_travel_draft, combined_duplicate_docids)
filtered_house_travel_draft <- bind_rows(filtered_house_travel_draft, consolidated_diff_sponsor_rows)
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, year_month) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_spons, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'year_month'))
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, sponsors_cleaned) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_month, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'sponsors_cleaned'))
no_change_diff_sponsor_month_rows <- subset(no_change_diff_sponsor_month_rows,
!(MemberName == "Beyer, Donald" &
FilingType == "Original" &
DepartureDate == "2021-11-19" &
sponsors_cleaned == "United Kingdom Marshall Scholars"))
possible_duplicates <- subset(no_change_diff_sponsor_month_rows,
sponsors_cleaned %in% c("Us-qatar Business Council",
"Hillsdale College",
"Washington Office On Latin America",
"Japan Center For International Exchange"))
filtered_no_change_diff_sponsor_month_rows <- anti_join(no_change_diff_sponsor_month_rows, possible_duplicates, by = c("DocID"))
filtered_house_travel_draft <- bind_rows(filtered_house_travel_draft, filtered_no_change_diff_sponsor_month_rows)
# Write the filtered house travel dataset to a CSV file
write_csv(filtered_house_travel_draft, "deduped_house_travel_full.csv")
# Write the remaining possible duplicates dataset to a CSV file.
write_csv(possible_duplicates, "possible_duplicates.csv")
house_current <- read_csv("house_current.csv")
# Extract the year from "departure_date" and populate "year" column
house_current$year <- year(house_current$departure_date)
# Extract the year/month combination from "departure_date" and populate "year_month" column
house_current$year_month <- paste(year(house_current$departure_date), month(house_current$departure_date, label = TRUE), sep = "-")
house_current %>%
group_by(sponsors_cleaned) %>%
summarise(count = n()) %>%
arrange(desc(count))
biofuels_filtered <- house_current[grepl("Renewable Fuels|Clean Fuels|Biodiesel|Biofuel", house_current$sponsors_cleaned, ignore.case = TRUE), ]
biofuels_filtered |>
arrange(desc(departure_date))
sugar_filtered <- house_current[grepl("Sugar|Sugarbeet|Sugarcane|South Florida Agricultural|Agricultural Institute of Florida|Louisiana Farm Bureau|Leadership Idaho Agriculture", house_current$sponsors_cleaned, ignore.case = TRUE), ]
sugar_filtered |>
arrange(desc(departure_date))
sugar_filtered |>
group_by(sponsors_cleaned) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_travelers <- sugar_filtered |>
filter(year >= 2016) |>
group_by(cleaned_filer_names) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
print(sugar_travelers)
write_csv(sugar_travelers, "sugar/sugar_travelers.csv")
sugar_filtered |>
group_by(member_name) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_red_river <- sugar_filtered[str_detect(sugar_filtered$sponsors_cleaned, "Red River"), ]
sugar_red_river |>
group_by(year)|>
summarise(count = n()) |>
arrange(desc(count))
sugar_filtered |>
group_by(state) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
# Convert the "Date" column to Date type if it's not already
sugar_filtered$date <- as.Date(sugar_filtered$departure_date)
# Create a line chart
sugar_filtered_plot <- ggplot(sugar_filtered, aes(x = departure_date)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored in Whole or in Part by Sugar Interests",
x = "Date",
y = "Trips") +
theme_minimal()
sugar_filtered_plot
# Convert the "Date" column to Date type if it's not already
sugar_red_river$date <- as.Date(sugar_red_river$departure_date)
# Create a line chart
sugar_red_river_plot <- ggplot(sugar_red_river, aes(x = departure_date)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored in Whole or in Part by Sugar Interests",
x = "Date",
y = "Trips") +
theme_minimal()
sugar_red_river_plot
sugar_filtered <- sugar_filtered %>%
mutate(Month = format(departure_date, "%Y-%m"))
sugar_dests_by_month_filtered <- sugar_filtered %>%
group_by(Month, combined_dests) %>%
summarise(count = n()) %>%
arrange(desc(count))
top5_per_month_sugar_filtered <- sugar_dests_by_month_filtered %>%
group_by(Month) %>%
top_n(5, count)
top5_per_month_sugar_filtered |>
arrange(desc(Month))
# Filter rows to isolate trips sponsored in whole or in part by other agricultural interests.
other_ag <- house_current[grepl("St. Louis Agribusiness Club|Farm Journal Foundation|National Association of State Departments of Agriculture|Farm Credit of the Virginias|Bowery Farming|Massachusetts Farm Bureau|National Farmers Union|National Grain and Feed Association|North Carolina Vegetable Growers Association|Livestock Marketing Association|Farm Foundation", house_current$sponsors_cleaned, ignore.case = TRUE), ]
other_ag|>
arrange(desc(departure_date))
# Filter rows to isolate trips sponsored by conservation or clean energy organizations that may have an interest in the sugar industry or its environmental impacts.
conservation_sugar_regions_narrow <- house_current[grepl("The Everglades Foundation|National Parks Conservation Association|Center for Clean Air Policy|Coalition to Restore Coastal Louisiana|Idaho Conservation League", house_current$sponsors_cleaned, ignore.case = TRUE), ]
conservation_sugar_regions_narrow|>
group_by(member_name) |>
summarize(count = n()) |>
arrange(desc(count))
print(conservation_sugar_regions_narrow)
# Filter rows to isolate trips sponsored by conservation or clean energy organizations that may have an interest in the sugar industry or its environmental impacts.
everglades_foundation <- house_current[grepl("The Everglades Foundation", house_current$sponsors_cleaned, ignore.case = TRUE), ]
everglades_foundation|>
group_by(member_name) |>
summarize(count = n()) |>
arrange(desc(count))
print(everglades_foundation)
everglades_foundation$Date <- as.Date(everglades_foundation$departure_date)
# Create a line chart
everglades_foundation_plot <- ggplot(everglades_foundation, aes(x = departure_date)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored by the Everglades Foundation",
x = "Date",
y = "Trips") +
theme_minimal()
everglades_foundation_plot
louisiana_sugar <- subset(sugar_filtered, grepl("Louisiana Sugar", sponsors_cleaned, ignore.case = TRUE))
# Display the resulting subset
print(louisiana_sugar)
louisiana_sugar |>
group_by(year) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
louisiana_sugar |>
filter(year>2014 & year<2018) |>
group_by(member_name, year) |>
summarize(trip_count = n()) |>
arrange(desc(year))
louisiana_sugar |>
group_by(state) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
midwest_sugar <- sugar_filtered[grepl("Sugarbeet|Leadership Idaho Agriculture", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
florida_sugar <- sugar_filtered[grepl("Florida", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
ag_inst_florida_sugar <- sugar_filtered[grepl("Agricultural Institute Of Florida Foundation", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
midwest_sugar |>
group_by(year) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
sugar_filtered |>
group_by(member_name) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
biofuels_filtered |>
group_by(member_name) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
write_csv(sugar_filtered, "sugar/sugar_trips_df.csv")
relevant_sugar_committees <- read_csv("sugar/relevant_sugar_committees.csv")
relevant_sugar_committees <- clean_names(relevant_sugar_committees)
# Calculate the percentage of rows where house_ag_committee is "Y"
percentage_ag_committee <- mean(relevant_sugar_committees$house_ag_committee == "Y") * 100
print(percentage_ag_committee)
pct_ag_committee_all_cong <- (54/435)*100
# Step 1: Data Preprocessing
set.seed(123)
# Select variables
independent_variables <- c(
"avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later",
"pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999",
"pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960",
"pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd",
"poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020",
"ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020",
"ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015",
"medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)
dependent_variable <- "foreclosure_pc_2020"
# Subsetting data
selected_data <- pg_foreclosures_per_tract[c(independent_variables, dependent_variable)]
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
#install.packages('car')
##install.packages("glmnet")
##install.packages("neuralnet")
##install.packages("mlbench")
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for dividing into training and test sets.
library(caret)
#Needed for data pre-processing and evaluation
library(car)
#Needed for regularization
library(glmnet)
library(e1071)
#Needed to build SVM model
library(neuralnet)
#Needed for neural networks
library(nnet)
#Needed to plot neural networks
library(mlbench)
#Needed for RFE feature selection
