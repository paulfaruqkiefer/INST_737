---
title: "pg_foreclosure_svm"
author: "Paul Kiefer, Krehl Kasayan"
date: "2024-04-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
#install.packages('car')
##install.packages("glmnet")
##install.packages("neuralnet")
##install.packages("mlbench")
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for dividing into training and test sets.
library(caret)
#Needed for data pre-processing and evaluation
library(car)
#Needed for regularization
library(glmnet)
library(e1071) 
#Needed to build SVM model
library(neuralnet)
#Needed for neural networks
library(nnet)
#Needed to plot neural networks
library(mlbench)
#Needed for RFE feature selection
```

Read in our dataset: 

```{r}
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max=173)
```
For the feature selection process, we also need pg_foreclosures_per_tract_log_reg:
```{r}
pg_foreclosures_per_tract_log_reg <- read_csv("datasets/pg_foreclosures_per_tract_log_reg.csv", guess_max=173)
```


First, we build an SVM using a regression approach and the continuous variable foreclosure_pc_2020.

```{r}
# Step 1: Data Preprocessing
set.seed(123)

# Select variables
independent_variables <- c(
  "avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020", 
  "tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later", 
  "pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999", 
  "pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960", 
  "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd", 
  "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020", 
  "mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010", 
  "ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015", 
  "mortgage_change_2015_2020", "mortgage_change_2010_2020", 
  "ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020", 
  "ownoccupied_change_2010_2020", "poverty_change_2010_2020", 
  "nhwhite_change_2010_2020", "medincome_change_2010_2015", 
  "medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)
dependent_variable <- "foreclosure_pc_2020"

# Subsetting data
selected_data <- pg_foreclosures_per_tract[c(independent_variables, dependent_variable)]

# Splitting data
train_index <- sample(nrow(selected_data), 0.8 * nrow(selected_data))
train_data_1 <- selected_data[train_index, ]
test_data_1 <- selected_data[-train_index, ]

# Step 2: Model Training
svm_model <- svm(as.formula(paste(dependent_variable, "~ .")), data = train_data_1, kernel ="linear")

# Step 3: Model Evaluation
predictions <- predict(svm_model, test_data_1)

# Calculate evaluation metrics
mse <- mean((test_data_1[[dependent_variable]] - predictions)^2)
rmse <- sqrt(mse)
rsquared <- cor(predictions, test_data_1[[dependent_variable]])^2

# Print evaluation metrics
print(paste("Mean Squared Error (MSE):", mse))
print(paste("Root Mean Squared Error (RMSE):", rmse))
print(paste("R-squared (R²):", rsquared))
```

To test the significance of that RSME, we can compare it to the standard deviation of the dependent variable.

```{r}
# Calculate the residuals
residuals <- test_data_1[[dependent_variable]] - predictions

# Calculate RMSE
RMSE <- sqrt(mean(residuals^2))

# Check if the RMSE is significantly different from the standard deviation
# Using a t-test
t_test_result <- t.test(test_data_1[[dependent_variable]], predictions)

# Check the p-value from the t-test
p_value <- t_test_result$p.value

# Check if p-value is less than a significance level (e.g., 0.05)
significance_level <- 0.05
if (p_value < significance_level) {
  print("RMSE is statistically significant.")
} else {
  print("RMSE is not statistically significant.")
}

print(p_value)
```
In other words, the RSME of the first SVM is not so substantial that the difference between the predicted and actual values has a reasonable chance of occurring due to random chance alone. 

Next, we create a column called foreclosure_hi_med_low to the dataframe and test a version of the SVM that uses a classifier rather than a regression. 

```{r}
# Add a new column 'foreclosure_hi_med_low' to the dataframe
pg_foreclosures_per_tract$foreclosure_hi_med_low <- cut(pg_foreclosures_per_tract$foreclosure_pc_2020,
                                                        breaks = quantile(pg_foreclosures_per_tract$foreclosure_pc_2020, probs = c(0, 1/3, 2/3, 1)),
                                                        labels = c(1, 2, 3),
                                                        include.lowest = TRUE)

# Convert the new column to factor
pg_foreclosures_per_tract$foreclosure_hi_med_low <- as.factor(pg_foreclosures_per_tract$foreclosure_hi_med_low)

# Select the list of independent variables
independent_variables <- c(
  "avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020", 
  "tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later", 
  "pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999", 
  "pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960", 
  "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd", 
  "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020", 
  "mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010", 
  "ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015", 
  "mortgage_change_2015_2020", "mortgage_change_2010_2020", 
  "ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020", 
  "ownoccupied_change_2010_2020", "poverty_change_2010_2020", 
  "nhwhite_change_2010_2020", "medincome_change_2010_2015", 
  "medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)

# Train-test split
set.seed(123)
train_index <- createDataPartition(pg_foreclosures_per_tract$foreclosure_hi_med_low, p = 0.8, list = FALSE)
train_data_2 <- pg_foreclosures_per_tract[train_index, ]
test_data_2 <- pg_foreclosures_per_tract[-train_index, ]

# Subsetting data with only selected variables
selected_train_data_2 <- train_data_2[c(independent_variables, "foreclosure_hi_med_low")]
selected_test_data_2 <- test_data_2[c(independent_variables, "foreclosure_hi_med_low")]

# Train SVM model for classification
svm_model_classification <- svm(foreclosure_hi_med_low ~ ., data = selected_train_data_2, kernel = "linear")

# Predictions on test set
predictions_classification <- predict(svm_model_classification, selected_test_data_2)

# Evaluate classification performance
conf_matrix <- confusionMatrix(predictions_classification, selected_test_data_2$foreclosure_hi_med_low)

# Calculate precision, recall, specificity, and F1 measure for each class
precision <- diag(conf_matrix$table) / colSums(conf_matrix$table)
recall <- diag(conf_matrix$table) / rowSums(conf_matrix$table)
specificity <- diag(conf_matrix$table[2:3, 2:3]) / colSums(conf_matrix$table[2:3, 2:3])
f1_measure <- 2 * (precision * recall) / (precision + recall)

# Print the confusion matrix
print(conf_matrix)

# Print precision, recall, specificity, and F1 measure for each class
cat("\nPrecision:\n", precision, "\n")
cat("Recall:\n", recall, "\n")
cat("Specificity:\n", specificity, "\n")
cat("F1 Measure:\n", f1_measure, "\n")


```
**NON-LINEAR KERNELS**

We will test both the regression and classification models with RBF and polynomial kernels.

First, the regression SVM with an RBF kernel:

```{r}
# Train SVM model with RBF kernel for regression
svm_model_rbf_regression <- svm(foreclosure_pc_2020 ~ ., data = train_data_1, kernel = "radial")

# Predictions on test set
predictions_rbf_regression <- predict(svm_model_rbf_regression, test_data_1)

# Calculate evaluation metrics (RMSE for regression)
rmse_rbf_regression <- sqrt(mean((test_data_1$foreclosure_pc_2020 - predictions_rbf_regression)^2))
print(paste("Root Mean Squared Error (RMSE) for RBF Kernel (Regression):", rmse_rbf_regression))
```

This RMSE is slightly larger than the RSME of the model with a linear kernel. 

To test the significance of that RMSE, we compare it to the standard deviation of foreclosure_pc_2020. 

```{r}
# Predictions on test set
predictions_rbf_regression <- predict(svm_model_rbf_regression, test_data_1)

# Calculate the residuals
residuals_rbf_regression <- test_data_1$foreclosure_pc_2020 - predictions_rbf_regression

# Calculate RMSE
rmse_rbf_regression <- sqrt(mean(residuals_rbf_regression^2))
print(paste("Root Mean Squared Error (RMSE) for RBF Kernel (Regression):", rmse_rbf_regression))

# Check if the RMSE is significantly different from the standard deviation
# Using a t-test
t_test_result <- t.test(test_data_1$foreclosure_pc_2020, predictions_rbf_regression)

# Check the p-value from the t-test
p_value <- t_test_result$p.value

# Check if p-value is less than a significance level (e.g., 0.05)
significance_level <- 0.05
if (p_value < significance_level) {
  print("RMSE is statistically significant.")
} else {
  print("RMSE is not statistically significant.")
}

print(p_value)
```

While the comparison does not provide evidence to disprove the null hypothesis, it is still not a stronger model than the model with a linear kernel.

Next, the regression SVM Model with a polynomial kernel. 

```{r}
# Train SVM model with Polynomial kernel for regression
svm_model_poly <- svm(foreclosure_pc_2020 ~ ., data = train_data_1, kernel = "polynomial")

# Predictions on test set
predictions_poly <- predict(svm_model_poly, test_data_1)

# Calculate evaluation metrics (RMSE for regression)
rmse_poly <- sqrt(mean((test_data_1$foreclosure_pc_2020 - predictions_poly)^2))
print(paste("Root Mean Squared Error (RMSE) for Polynomial Kernel:", rmse_poly))
```

This RMSE is also slightly higher than the RMSE of the model with a linear kernel. 

We can check the significance of that RMSE by comparing it to the standard deviation and range of foreclosure_pc_2020. 
```{r}
# Train SVM model with Polynomial kernel for regression
svm_model_poly <- svm(foreclosure_pc_2020 ~ ., data = train_data_1, kernel = "polynomial")

# Predictions on test set
predictions_poly <- predict(svm_model_poly, test_data_1)

# Calculate evaluation metrics (RMSE for regression)
rmse_poly <- sqrt(mean((test_data_1$foreclosure_pc_2020 - predictions_poly)^2))
print(paste("Root Mean Squared Error (RMSE) for Polynomial Kernel:", rmse_poly))

# Check if the RMSE is significantly different from the standard deviation
# Using a t-test
t_test_result <- t.test(test_data_1$foreclosure_pc_2020, predictions_poly)

# Check the p-value from the t-test
p_value <- t_test_result$p.value

# Check if p-value is less than a significance level (e.g., 0.05)
significance_level <- 0.05
if (p_value < significance_level) {
  print("RMSE is statistically significant.")
} else {
  print("RMSE is not statistically significant.")
}

print(p_value)
```
While the comparison does not provide evidence to disprove the null hypothesis, it is still not a stronger model than both the model with a linear kernel and the model with the RBF kernel.

Next, we apply both the RBF and polynomial kernels to the classification SVM.

SVM Classification with RBF Kernel
```{r}
# Train-test split
set.seed(123)
train_index <- createDataPartition(pg_foreclosures_per_tract$foreclosure_hi_med_low, p = 0.8, list = FALSE)
train_data_2 <- pg_foreclosures_per_tract[train_index, ]
test_data_2 <- pg_foreclosures_per_tract[-train_index, ]

# Subsetting data with only selected variables
selected_train_data_2 <- train_data_2[c(independent_variables, "foreclosure_hi_med_low")]
selected_test_data_2 <- test_data_2[c(independent_variables, "foreclosure_hi_med_low")]

# Train SVM model for classification with RBF kernel
svm_model_classification_rbf <- svm(foreclosure_hi_med_low ~ ., data = selected_train_data_2, kernel = "radial")

# Predictions on test set
predictions_classification_rbf <- predict(svm_model_classification_rbf, selected_test_data_2)

# Evaluate classification performance
conf_matrix_rbf <- confusionMatrix(predictions_classification_rbf, selected_test_data_2$foreclosure_hi_med_low)

# Calculate precision, recall, specificity, and F1 measure for each class
precision_rbf <- diag(conf_matrix_rbf$table) / colSums(conf_matrix_rbf$table)
recall_rbf <- diag(conf_matrix_rbf$table) / rowSums(conf_matrix_rbf$table)
specificity_rbf <- diag(conf_matrix_rbf$table[2:3, 2:3]) / colSums(conf_matrix_rbf$table[2:3, 2:3])
f1_measure_rbf <- 2 * (precision_rbf * recall_rbf) / (precision_rbf + recall_rbf)

# Print the confusion matrix
print(conf_matrix_rbf)

# Print precision, recall, specificity, and F1 measure for each class
cat("\nPrecision:\n", precision_rbf, "\n")
cat("Recall:\n", recall_rbf, "\n")
cat("Specificity:\n", specificity_rbf, "\n")
cat("F1 Measure:\n", f1_measure_rbf, "\n")

```
Overall, this model is less accurate than the model with the linear kernel.

SVM classification model with polynomial kernel;
```{r}
# Train SVM model for classification with polynomial kernel
svm_model_classification_poly <- svm(foreclosure_hi_med_low ~ ., data = selected_train_data_2, kernel = "polynomial")

# Predictions on test set
predictions_classification_poly <- predict(svm_model_classification_poly, selected_test_data_2)

# Evaluate classification performance
conf_matrix_poly <- confusionMatrix(predictions_classification_poly, selected_test_data_2$foreclosure_hi_med_low)

# Calculate precision, recall, specificity, and F1 measure for each class
precision_poly <- diag(conf_matrix_poly$table) / colSums(conf_matrix_poly$table)
recall_poly <- diag(conf_matrix_poly$table) / rowSums(conf_matrix_poly$table)
specificity_poly <- diag(conf_matrix_poly$table[2:3, 2:3]) / colSums(conf_matrix_poly$table[2:3, 2:3])
f1_measure_poly <- 2 * (precision_poly * recall_poly) / (precision_poly + recall_poly)

# Print the confusion matrix
print(conf_matrix_poly)

# Print precision, recall, specificity, and F1 measure for each class
cat("\nPrecision:\n", precision_poly, "\n")
cat("Recall:\n", recall_poly, "\n")
cat("Specificity:\n", specificity_poly, "\n")
cat("F1 Measure:\n", f1_measure_poly, "\n")

```
While this model is slightly more accurate than the model with the RBF kernel, it is less accurate in all quantiles than the model with the linear kernel. 



**NEURAL NETWORKS**

We test Neural Networks with 10 multiple configurations (i.e. activaion functions, number of hidden layers, numbers of neurons per layer):
  
```{r}
# Select the list of independent variables and the dependent variable
independent_variables <- c(
  "avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020", 
  "tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later", 
  "pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999", 
  "pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960", 
  "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd", 
  "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020", 
  "mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010", 
  "ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015", 
  "mortgage_change_2015_2020", "mortgage_change_2010_2020", 
  "ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020", 
  "ownoccupied_change_2010_2020", "poverty_change_2010_2020", 
  "nhwhite_change_2010_2020", "medincome_change_2010_2015", 
  "medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)

# Scale from 0-1
min_max_scaling <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

# Extract the selected columns from the dataframe
selected_data <- pg_foreclosures_per_tract[independent_variables]

# Normalize the selected columns
selected_data_normalized <- selected_data
selected_data_normalized[] <- lapply(selected_data, min_max_scaling)

# Add foreclosure_pc_2020 to the normalized dataframe
selected_data_normalized$foreclosure_pc_2020 <- min_max_scaling(pg_foreclosures_per_tract$foreclosure_pc_2020)

# Split data into training and testing sets
set.seed(123)
train_index <- createDataPartition(selected_data_normalized$foreclosure_pc_2020, p = 0.8, list = FALSE)
train_data_4 <- selected_data_normalized[train_index, ]
test_data_4 <- selected_data_normalized[-train_index, ]

# Define configurations to test
configurations <- list(
  list(hidden_layers = 1, neurons_per_layer = c(10), activation_function = "logistic"),
  list(hidden_layers = 2, neurons_per_layer = c(10, 5), activation_function = "tanh"),
  list(hidden_layers = 1, neurons_per_layer = c(5), activation_function = "logistic"),
  list(hidden_layers = 2, neurons_per_layer = c(5, 3), activation_function = "tanh"),
  list(hidden_layers = 3, neurons_per_layer = c(8, 6, 4), activation_function = "logistic"),
  list(hidden_layers = 3, neurons_per_layer = c(12, 8, 4), activation_function = "tanh"),
  list(hidden_layers = 4, neurons_per_layer = c(10, 8, 6, 4), activation_function = "logistic"),
  list(hidden_layers = 4, neurons_per_layer = c(12, 10, 6, 3), activation_function = "tanh"),
  list(hidden_layers = 5, neurons_per_layer = c(15, 12, 10, 8, 6), activation_function = "logistic"),
  list(hidden_layers = 5, neurons_per_layer = c(20, 15, 12, 8, 5), activation_function = "tanh")
)


# Function to build and train neural network model
train_nn_model <- function(hidden_layers, neurons_per_layer, activation_function, train_data_4, test_data_4) {
  # Build neural network model
  model <- neuralnet(
    foreclosure_pc_2020 ~ .,
    data = train_data_4,  # Use normalized training data
    hidden = neurons_per_layer,
    linear.output = TRUE,  # For regression tasks
    act.fct = activation_function
  )
  
  # Make predictions on test data
  predictions <- predict(model, test_data_4)
  
  # Calculate evaluation metrics (e.g., RMSE)
  rmse <- sqrt(mean((test_data_4$foreclosure_pc_2020 - predictions)^2))
  
  # Return performance results
  return(list(
    rmse = rmse,
    activation_function = activation_function,
    hidden_layers = hidden_layers,
    neurons_per_layer = neurons_per_layer,
    model = model
  ))
}

# Evaluate neural network models for each configuration
results <- lapply(configurations, function(config) {
  train_nn_model(config$hidden_layers, config$neurons_per_layer, config$activation_function, train_data_4, test_data_4)
})


# Report results
for (i in seq_along(results)) {
  cat("Configuration:", i, "\n")
  cat("RMSE:", results[[i]]$rmse, "\n")
  cat("Activation Function:", results[[i]]$activation_function, "\n")
  cat("Hidden Layers:", results[[i]]$hidden_layers, "\n")
  cat("Neurons per Layer:", results[[i]]$neurons_per_layer, "\n\n")
}

# Identify the best configuration based on performance metrics
best_config <- configurations[[which.min(sapply(results, function(x) x$rmse))]]

# Train the best model with the identified configuration
best_model <- train_nn_model(best_config$hidden_layers, best_config$neurons_per_layer, best_config$activation_function, train_data_4 = train_data_4, test_data_4 = test_data_4)
```
The best-performing NN is configuration 7, plotted here.

```{r}
plot(best_model$model, cex = 0.8) 
```

**FEATURE SELECTION**

We apply step and RFE feature selection on three relatively inaccurate models:


1. Regression SVM with linear kernel and feature selection using the filter method:

```{r}
# Step 1: Data Preprocessing
set.seed(123)

# Select variables
independent_variables <- c(
  "avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020", 
  "tract_medincome_2010", "foreclosure_pc_2010", "pct_built_2020_later", 
  "pct_built_2010_2019", "pct_built_2000_2009", "pct_built_1990_1999", 
  "pct_built_1980_1989", "pct_built_1970_1979", "pct_built_pre_1960", 
  "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", "pct_4_more_bd", 
  "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020", 
  "mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010", 
  "ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015", 
  "mortgage_change_2015_2020", "mortgage_change_2010_2020", 
  "ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020", 
  "ownoccupied_change_2010_2020", "poverty_change_2010_2020", 
  "nhwhite_change_2010_2020", "medincome_change_2010_2015", 
  "medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct"
)
dependent_variable <- "foreclosure_pc_2020"

# Subsetting data
selected_data <- pg_foreclosures_per_tract[c(independent_variables, dependent_variable)]

# Calculate correlations between independent variables and dependent variable
correlations <- cor(selected_data[independent_variables], selected_data[[dependent_variable]])

# Get the names of the variables from the correlation matrix
variable_names <- rownames(correlations)

# Sort the correlations matrix by absolute values
sorted_correlations <- sort(abs(correlations), decreasing = TRUE, index.return = TRUE)

# Extract the row names of the top N variables
N <- 10  # Number of top variables to select
top_indices <- sorted_correlations$ix[1:N]
most_predictive_variables <- rownames(correlations)[top_indices]

# Calculate VIF for each variable
library(car)
vif_results <- vif(lm(selected_data[, most_predictive_variables]))

# Identify variables with VIF values over 10
high_vif_variables <- names(vif_results[vif_results > 10])


# Calculate correlation coefficients w/ the high VIF variables and the dependent variable
correlation_with_foreclosure <- sapply(high_vif_variables, function(var) {
  cor(pg_foreclosures_per_tract[[var]], pg_foreclosures_per_tract[[dependent_variable]])
})

# Find the variable with the highest absolute correlation coefficient
most_correlated_variable_high_vif <- high_vif_variables[which.max(abs(correlation_with_foreclosure))]

# Drop variables from high_vif_variables from most_predictive_variables
most_predictive_variables <- setdiff(most_predictive_variables, high_vif_variables)

# Add most_correlated_variable_high_vif to most_predictive_variables
most_predictive_variables <- c(most_predictive_variables, most_correlated_variable_high_vif)

# Subsetting data after feature selection
train_data_filtered <- pg_foreclosures_per_tract[train_index, ]
test_data_filtered <- pg_foreclosures_per_tract[-train_index, ]

# Step 2: Model Training
svm_model <- svm(as.formula(paste(dependent_variable, "~ .")), data = train_data_filtered, kernel ="linear")

# Step 3: Model Evaluation
predictions <- predict(svm_model, test_data_filtered)

# Calculate evaluation metrics
mse <- mean((test_data_filtered[[dependent_variable]] - predictions)^2)
rmse <- sqrt(mse)
rsquared <- cor(predictions, test_data_filtered[[dependent_variable]])^2

# Print evaluation metrics
print(paste("Mean Squared Error (MSE):", mse))
print(paste("Root Mean Squared Error (RMSE):", rmse))
print(paste("R-squared (R²):", rsquared))

```
Applying filter feature selection -- albeit in this (possibly) unnecessarily complicated way -- drastically improves the predictiveness of the regression SVM by selecting only six of the 40 independent variables to train the model. 


2. Naive Bayes model using the wrapper method of feature selection (stepwise function):
```{r}
# Convert foreclosure_quantile to a factor with ordered levels
pg_foreclosures_per_tract_log_reg$foreclosure_quantile <- factor(
  pg_foreclosures_per_tract_log_reg$foreclosure_quantile,
  levels = 1:5,
  ordered = TRUE
)

set.seed(123) # For reproducibility
test_percent <- 0.2
indices <- sample(1:nrow(pg_foreclosures_per_tract_log_reg), 
                  size = round(test_percent * nrow(pg_foreclosures_per_tract_log_reg)))

train_data_2 <- pg_foreclosures_per_tract_log_reg[-indices, ]
test_data_2 <- pg_foreclosures_per_tract_log_reg[indices, ]

# Extracting independent variables
independent_variables <- c(
  "nhwhite_2020", "foreclosure_pc_2010", "tract_medage_2020", 
  "poverty_2010", "medincome_change_2010_2020", "avg_bed", 
  "mortgaged_2010", "pct_1_bd", "pct_built_pre_1960", 
  "pct_built_2000_2009", "mortgage_change_2010_2015",
  "pop_change_pct"
)

# Fit logistic regression model with stepwise feature selection
stepwise_model <- step(
  glm(foreclosure_quantile ~ ., 
      data = train_data_2[, c(independent_variables, "foreclosure_quantile")],
      family = binomial),
  direction = "both"
)

# Extract selected variables
selected_variables <- attr(terms(stepwise_model), "term.labels")

# Fit Naive Bayes model with selected variables
selected_naive_bayes_model <- naiveBayes(
  formula = as.formula(paste("foreclosure_quantile ~", paste(selected_variables, collapse = " + "))),
  data = train_data_2
)

# Predict using the Naive Bayes model
predictions <- predict(selected_naive_bayes_model, newdata = test_data_2)

# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data_2$foreclosure_quantile)

# Print the confusion matrix
print(confusion_matrix)

```
After applying the stepwise function for feature selection, our Naive Bayes model is actually less accurate than the original Naive Bayes model from Milestone 2, albeit only by roughly 3 percent. The original model more accurately predicted Class 5 (the highest foreclosure rate per capita) than this model with feature selection. 



3. Multivariate Linear Regression updated with embedded feature selection (L2 regularization)
```{r}
library(caret)
library(glmnet)

variables_of_interest <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020", 
                            "tract_medincome_2010", "foreclosure_pc_2010", 
                            "pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009", 
                            "pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979", 
                            "pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd", 
                            "pct_4_more_bd", "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020", 
                            "mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010", 
                            "ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015", 
                            "mortgage_change_2015_2020", "mortgage_change_2010_2020","ownoccupied_change_2010_2015",                             "ownoccupied_change_2015_2020", "ownoccupied_change_2010_2020",
                            "poverty_change_2010_2020","nhwhite_change_2010_2020","medincome_change_2010_2015",
                            "medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct")

# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- createDataPartition(train_data_6$foreclosure_pc_2020, p = 0.8, list = FALSE)
train_data <- train_data_6[train_indices, ]
test_data <- train_data_6[-train_indices, ]

# Train the Ridge regression model on the training data with selected features
ridge_model <- glmnet(x = as.matrix(train_data[, variables_of_interest]), 
                      y = train_data$foreclosure_pc_2020, 
                      alpha = 0, # Ridge regression (L2 regularization)
                      lambda = 1) # Regularization parameter, can be tuned using cross-validation

# Use the trained model to predict on the testing data with selected features
predicted_values <- predict(ridge_model, newx = as.matrix(test_data[, variables_of_interest]))

# Calculate the correlation between the predicted and real values
correlation <- cor(predicted_values, test_data$foreclosure_pc_2020)

# Calculate the mean squared error between the predicted and real values
mse <- mean((predicted_values - test_data$foreclosure_pc_2020)^2)

# Print the correlation and mean squared error
cat("Correlation between predicted and real values:", correlation, "\n")
cat("Mean Squared Error:", mse, "\n")
```
The linear regression model with all of our independent variables is modestly more predictive after undergoing L2 regularization (i.e. updated with Ridge regression). The previous version of this model had the following output: 

Correlation between predicted and real values: 0.7934256 
Mean Squared Error: 437.2731 

In other words, the correlation between the predicted and real values increased by just over 5 percent, though the MSE decreased more substantially.
